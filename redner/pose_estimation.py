# -*- coding: utf-8 -*-
"""pose_estimation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/BachiLi/redner/blob/master/tutorials/pose_estimation.ipynb

In this tutorial we will use redner to optimize the translational and rotational parameters of an object.

As in the previous tutorial, we will import pyredner and pytorch, and download the teapot object.
"""


import pyredner
import torch

# import urllib
# import zipfile
# wget
# filedata = urllib.request.urlretrieve('https://casual-effects.com/g3d/data10/common/model/teapot/teapot.zip', 'teapot.zip')
# # unzip
# zip_ref = zipfile.ZipFile('teapot.zip', 'r')
# zip_ref.extractall('teapot/')

objects = pyredner.load_obj('mori_knob/testObj.obj', return_objects=True)
objects = [objects[0]]

camera = pyredner.automatic_camera_placement(objects, resolution=(512, 512))

"""Next, we define a `model` function that takes the objects, camera, and pose parameters, and output an image."""

# Obtain the teapot vertices we want to apply the transformation on.
vertices = []
for obj in objects:
    vertices.append(obj.vertices.clone())
# Compute the center of the teapot
center = torch.mean(torch.cat(vertices), 0)
def model(translation, euler_angles):
    # Get the rotation matrix from Euler angles
    rotation_matrix = pyredner.gen_rotate_matrix(euler_angles)
    # Shift the vertices to the center, apply rotation matrix,
    # shift back to the original space, then apply the translation.
    for obj, v in zip(objects, vertices):
        obj.vertices = (v - center) @ torch.t(rotation_matrix) + center + translation
    # Assemble the 3D scene.
    scene = pyredner.Scene(camera = camera, objects = objects)
    # Render the scene.
    img = pyredner.render_albedo(scene)
    return img

"""To do pose estimation, we need a target image. In this toy example we use the default pose to render a target image."""

# pyredner.get_device() returns the default device used by pyredner depending on whether CUDA is available.
# use pyredner.set_use_gpu(Boolean) or pyredner.set_device(torch.Device) to change it.
target_translation = torch.tensor([0.0, 0.0, 0.0], device = pyredner.get_device())
target_euler_angles = torch.tensor([0.0, 0.0, 0.0], device = pyredner.get_device())

target = model(target_translation, target_euler_angles).data
print(target)

# Commented out IPython magic to ensure Python compatibility.
# Visualize target
from matplotlib.pyplot import imshow
# %matplotlib inline
# Need to gamma compress the image for displaying.
imshow(torch.pow(target, 1.0/2.2).cpu())

"""Now we generate the initial guess for the pose estimation."""

# Set requires_grad=True since we want to optimize them later
translation = torch.tensor([10.0, -10.0, 10.0], device = pyredner.get_device(), requires_grad=True)
euler_angles = torch.tensor([0.1, -0.1, 0.1], device = pyredner.get_device(), requires_grad=True)
init = model(translation, euler_angles)
# Visualize the initial guess
imshow(torch.pow(init.data, 1.0/2.2).cpu()) # add .data to stop PyTorch from complaining

"""Next, we optimize the pose using Adam. We build two optimizers to have different learning rates for translation and rotation, since they have different dynamic ranges."""

t_optimizer = torch.optim.Adam([translation], lr=0.5)
r_optimizer = torch.optim.Adam([euler_angles], lr=0.01)

# Commented out IPython magic to ensure Python compatibility.
# Setup plotting
import matplotlib.pyplot as plt
# %matplotlib inline
from IPython.display import display, clear_output
import time
plt.figure()
imgs, losses = [], []
# Run 80 Adam iterations
num_iters = 10
for t in range(num_iters):
    t_optimizer.zero_grad()
    r_optimizer.zero_grad()
    img = model(translation, euler_angles)
    # Compute the loss function. Here it is L2.
    # Both img and target are in linear color space, so no gamma correction is needed.
    loss = (img - target).pow(2).mean()
    loss.backward()
    t_optimizer.step()
    r_optimizer.step()
    # Plot the loss
    if t == num_iters - 1:
        f, (ax_loss, ax_img) = plt.subplots(1, 2)
    losses.append(loss.data.item())
    imgs.append(torch.pow(img.data, 1.0/2.2).cpu()) # Record the Gamma corrected image
    clear_output(wait=True)

ax_loss.plot(range(len(losses)), losses, label='loss')
ax_loss.legend()
ax_img.imshow((img -target).pow(2).sum((2)).data.cpu())
plt.show()

"""We can visualize the intermediate images using matplotlib:"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
from matplotlib import animation
from IPython.display import HTML
fig = plt.figure()
im = plt.imshow(imgs[0], animated=True)
def update_fig(i):
    im.set_array(imgs[i])
    return im,
anim = animation.FuncAnimation(fig, update_fig, frames=len(imgs), interval=50, blit=True)

from IPython.display import HTML
HTML(anim.to_jshtml())